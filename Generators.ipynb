{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n1: Param\n",
      "n2: Param\n",
      "n17: Map\n",
      "[('n1', 'n17')]\n",
      "\n",
      "       Subgraph of Map node  n17\n",
      "n3: Param\n",
      "n15: Map\n",
      "n16: Transpose\n",
      "[('n16', 'n15')]\n",
      "[('n2', 'n16')]\n",
      "\n",
      "       Subgraph of Map node  n15\n",
      "n4: Param\n",
      "n8: Reduce\n",
      "n13: Map\n",
      "n14: Zip\n",
      "[('n13', 'n8')]\n",
      "[('n3', 'n14')]\n",
      "[('n4', 'n14')]\n",
      "[('n14', 'n13')]\n",
      "\n",
      "       Subgraph of Reduce node  n8\n",
      "n5: Mather\n",
      "n6: Param\n",
      "n7: Param\n",
      "[('n6', 'n5')]\n",
      "[('n7', 'n5')]\n",
      "\n",
      "       Subgraph of Map node  n13\n",
      "n9: Param\n",
      "n11: Get\n",
      "n12: Get\n",
      "n10: Mather\n",
      "[('n9', 'n11')]\n",
      "[('n11', 'n10')]\n",
      "[('n9', 'n12')]\n",
      "[('n12', 'n10')]\n"
     ]
    }
   ],
   "source": [
    "%run ParserLexer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidying functions\n",
    "\n",
    "# Remove back-to-back Rearray/Dearray pairs\n",
    "def compose_maps(g):\n",
    "    ns = g[\"nodes\"]\n",
    "    cs = g[\"channels\"]\n",
    "    n_dict = dict([(n.name, n) for n in ns])\n",
    "    rds = [ c for c in cs if ((getcn(n_dict[c.src_act]) == \"Dearray\" \n",
    "                        and getcn(n_dict[c.dst_act]) == \"Rearray\")\n",
    "                       or (getcn(n_dict[c.src_act]) == \"Rearray\" \n",
    "                        and getcn(n_dict[c.dst_act]) == \"Dearray\"))]\n",
    "    for rd in rds:\n",
    "        src = n_dict[rd.src_act]\n",
    "        dst = n_dict[rd.dst_act]\n",
    "        assert(getcn(src) in [\"Rearray\", \"Dearray\"])\n",
    "        assert(getcn(dst) in [\"Rearray\", \"Dearray\"])\n",
    "        if src.output.rate == dst.input.rate:\n",
    "            inc_c = [c for c in cs if  c.dst_port == src.input.name][0]\n",
    "            out_c = [c for c in cs if c.src_port == dst.output.name][0]\n",
    "            assert(str(inc_c.datatype) == str(out_c.datatype))\n",
    "            new_c = Channel(inc_c.src_act, out_c.dst_act, inc_c.src_port, out_c.dst_port)\n",
    "            new_c.add_dt(inc_c.datatype)\n",
    "            cs = [c for c in cs if c not in [inc_c, out_c, rd]] + [new_c]\n",
    "            ns = [n for n in ns if n not in [src, dst]]\n",
    "    g[\"nodes\"] = ns\n",
    "    g[\"channels\"] = cs\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation functions\n",
    "\n",
    "# Compile Recursive CSDF graph\n",
    "\n",
    "def exploder(methods):\n",
    "    return lambda g: explode(g, methods)\n",
    "\n",
    "def explode(graph, methods):\n",
    "    for method in methods:\n",
    "        method(graph, exploder(methods))\n",
    "    return graph\n",
    "\n",
    "\n",
    "def recursive_map(graph, explode_method):\n",
    "    nodes = graph[\"nodes\"]\n",
    "    channels = graph[\"channels\"]\n",
    "    def_len = 10\n",
    "    \n",
    "    # get Map nodes\n",
    "    mapnodes = [n for n in nodes if getcn(n) == \"Map\"]\n",
    "    # explode Map nodes\n",
    "    for node in mapnodes:\n",
    "        name = node.name\n",
    "        \n",
    "        old_in_channel = [c for c in channels if c.dst_act == name]\n",
    "        old_out_channel = [c for c in channels if c.src_act == name]\n",
    "        assert(len(old_in_channel) == 1)\n",
    "        assert(len(old_out_channel) <= 1)\n",
    "        \n",
    "        da_in = node.input\n",
    "        da_out = Port(\"out\", name + \"XDA_out\", [node.rep])\n",
    "        da = Dearray(name + \"XDA\", da_in, da_out)\n",
    "        da.datatype = old_in_channel[0].datatype.subdata\n",
    "        ra_out = node.output\n",
    "        ra_in = Port(\"in\", name + \"XRA_in\", [node.rep])\n",
    "        ra = Rearray(name + \"XRA\", ra_in, ra_out, \"Map\")\n",
    "        ra.datatype = node.datatype\n",
    "        \n",
    "        subgraph = explode_method(node.subfunc)\n",
    "        nodes += subgraph[\"nodes\"]\n",
    "        nodes += [da, ra]\n",
    "        \n",
    "        if len(old_in_channel) == 1:\n",
    "            old_in_channel[0].dst_act = da.name\n",
    "        if len(old_out_channel) == 1:\n",
    "            old_out_channel[0].src_act = ra.name\n",
    "        channels += subgraph[\"channels\"]\n",
    "        so = subgraph[\"output\"].name\n",
    "        si = subgraph[\"inputs\"][0].name\n",
    "        channels += [Channel(da.name, \n",
    "                             get_node_by_port(subgraph[\"nodes\"], si).name, \n",
    "                             da_out.name, si), \n",
    "                     Channel(get_node_by_port(subgraph[\"nodes\"], so).name, \n",
    "                             ra.name, so, ra_in.name)]\n",
    "    # remove Map nodes\n",
    "    nodes = [n for n in nodes if getcn(n) != \"Map\"]\n",
    "    \n",
    "    \n",
    "    graph[\"nodes\"] = nodes\n",
    "    graph[\"channels\"] = channels\n",
    "    return graph\n",
    "\n",
    "def recursive_reduce(graph, explode_method):\n",
    "    def_len = 10\n",
    "    nodes = graph[\"nodes\"]\n",
    "    channels = graph[\"channels\"]\n",
    "    \n",
    "    # get Reduce nodes\n",
    "    reducenodes = [n for n in nodes if getcn(n).startswith(\"Reduce\")]\n",
    "    # explode Reduce nodes\n",
    "    for node in reducenodes:\n",
    "        name = node.name\n",
    "        # get channels\n",
    "        old_in_channel = [c for c in channels if c.dst_act == name]\n",
    "        old_out_channel = [c for c in channels if c.src_act == name]\n",
    "        \n",
    "        # make new nodes\n",
    "        da_in = node.input\n",
    "        da_out = Port(\"out\", name + \"XDA_out\", [node.rep])\n",
    "        da = Dearray(name + \"XDA\", da_in, da_out)\n",
    "        da.datatype = old_in_channel[0].datatype.subdata\n",
    "        \n",
    "        ra_in = Port(\"in\", name + \"XRA_in\", [1])\n",
    "        ra_out = node.output\n",
    "        ra = Rearray(name + \"XRA\", ra_in, ra_out, \"Reduce\")\n",
    "        ra.datatype = node.datatype\n",
    "        \n",
    "        if len(old_in_channel) == 1:\n",
    "            old_in_channel[0].dst_act = da.name\n",
    "        if len(old_out_channel) == 1:\n",
    "            old_out_channel[0].src_act = ra.name\n",
    "        \n",
    "        # make param for recursion\n",
    "        para = Param(name + \"XPARA\", \"recursion\")\n",
    "        para.datatype = node.datatype.subdata\n",
    "        para.input.rate = [1] * node.rep\n",
    "        para_out_0 = para.new_outport([1] * node.rep)\n",
    "        para_out_1 = para.new_outport([0] * (node.rep - 1) + [1])\n",
    "        \n",
    "        # add nodes\n",
    "        subgraph = explode_method(node.subfunc)\n",
    "        nodes += subgraph[\"nodes\"]\n",
    "        nodes += [da, ra, para]\n",
    "        \n",
    "        # add channels\n",
    "        si = subgraph[\"inputs\"]\n",
    "        so = subgraph[\"output\"].name\n",
    "        channels += [Channel(para.name, get_node_by_port(subgraph[\"nodes\"], si[1].name).name, \n",
    "                             para_out_0.name, si[1].name, 1),\n",
    "                     Channel(da.name, get_node_by_port(subgraph[\"nodes\"], si[0].name).name, \n",
    "                             da_out.name, si[0].name),\n",
    "                     Channel(get_node_by_port(subgraph[\"nodes\"], so).name, para.name,\n",
    "                             so, para.input.name),\n",
    "                     Channel(para.name, ra.name,\n",
    "                             para_out_1.name, ra_in.name)]\n",
    "        channels += subgraph[\"channels\"]\n",
    "        \n",
    "    # remove Reduce nodes\n",
    "    nodes = [n for n in nodes if not getcn(n).startswith(\"Reduce\")]\n",
    "    \n",
    "    \n",
    "    graph[\"nodes\"] = nodes\n",
    "    graph[\"channels\"] = channels\n",
    "    return graph\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepcopy(g, i):\n",
    "    i = str(i)\n",
    "    newg = copy.deepcopy(g)\n",
    "    nodes = newg['nodes']\n",
    "    nodenames = [n.name for n in nodes]\n",
    "    for channel in newg['channels']:\n",
    "        if channel.src_act in nodenames and channel.dst_act in nodenames:\n",
    "            channel.name = channel.name + \"_x\" + i\n",
    "        if channel.src_act in nodenames:\n",
    "            channel.src_act = channel.src_act + \"_x\" + i\n",
    "            channel.src_port = channel.src_port + \"_x\" + i\n",
    "        if channel.dst_act in nodenames:\n",
    "            channel.dst_act = channel.dst_act + \"_x\" + i\n",
    "            channel.dst_port = channel.dst_port + \"_x\" + i\n",
    "    for node in nodes:\n",
    "        node.name = node.name + \"_x\" + i\n",
    "        for port in getports(node):\n",
    "            port.name = port.name + \"_x\" + i\n",
    "    return newg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_reduce(graph, explode_method, groupsize = 1):\n",
    "    def_len = 10\n",
    "    nodes = graph[\"nodes\"]\n",
    "    channels = graph[\"channels\"]\n",
    "    \n",
    "    # get Reduce nodes\n",
    "    reducenodes = [n for n in nodes if getcn(n) == (\"Reduce\")]\n",
    "    # explode Reduce nodes\n",
    "    for node in reducenodes:\n",
    "        name = node.name\n",
    "        # get channels\n",
    "        old_in_channel = [c for c in channels if c.dst_act == name]\n",
    "        old_out_channel = [c for c in channels if c.src_act == name]\n",
    "        inc_datatype = old_in_channel[0].datatype\n",
    "        \n",
    "        # make new nodes\n",
    "        da_in = node.input\n",
    "        da_out = Port(\"out\", name + \"XDA_out\", [node.rep])\n",
    "        da = Dearray(name + \"XDA\", da_in, da_out)\n",
    "        da.datatype = inc_datatype.subdata\n",
    "        \n",
    "        ra_in = Port(\"in\", name + \"XRA_in\", [1])\n",
    "        ra_out = node.output\n",
    "        ra = Rearray(name + \"XRA\", ra_in, ra_out, \"Reduce\")\n",
    "        ra.datatype = node.datatype\n",
    "        \n",
    "        if len(old_in_channel) == 1:\n",
    "            old_in_channel[0].dst_act = da.name\n",
    "        if len(old_out_channel) == 1:\n",
    "            old_out_channel[0].src_act = ra.name\n",
    "        \n",
    "        # make initial value\n",
    "        ival = Value(name + \"XInitVal\", node.initval, inc_datatype.subdata)\n",
    "        ival.add_dt(inc_datatype.subdata)\n",
    "        \n",
    "        print(inc_datatype)\n",
    "        \n",
    "        # make Splitter and link to dearray\n",
    "        total_vals =  inc_datatype.length\n",
    "        routes = int(total_vals / groupsize)\n",
    "        remaining = total_vals % groupsize\n",
    "        \n",
    "        spl = Splitter(name+\"XSpl\", routes, groupsize, remaining)\n",
    "        spl.add_dt(inc_datatype.subdata)\n",
    "        channels += [Channel(da.name, spl.name, da_out.name, spl.input.name)]\n",
    "        \n",
    "        # add nodes\n",
    "        nodes += [da, ra, ival, spl]\n",
    "        subgraph = explode_method(node.subfunc)\n",
    "        datavals = [(spl.name, p.name) for p in spl.output] + [(ival.name, ival.output.name)]\n",
    "        count = 0\n",
    "        while len(datavals) > 1:\n",
    "            sg = deepcopy(subgraph, count)\n",
    "            count += 1\n",
    "            nodes += sg[\"nodes\"]\n",
    "            channels += sg[\"channels\"]\n",
    "            # add channels\n",
    "            in1 = datavals.pop(0)\n",
    "            in2 = datavals.pop(0)\n",
    "            si = sg[\"inputs\"]\n",
    "            \n",
    "            channels += [Channel(in1[0], get_node_by_port(sg[\"nodes\"], si[0].name).name, \n",
    "                                 in1[1], si[0].name),\n",
    "                         Channel(in2[0], get_node_by_port(sg[\"nodes\"], si[1].name).name, \n",
    "                                 in2[1], si[1].name)]\n",
    "            so = sg[\"output\"].name\n",
    "            datavals.append((get_node_by_port(sg[\"nodes\"], so).name, so))\n",
    "        in1 = datavals.pop()\n",
    "        channels.append(Channel(in1[0], ra.name, \n",
    "                                 in1[1], ra_in.name))\n",
    "        \n",
    "    # remove Reduce nodes\n",
    "    nodes = [n for n in nodes if not getcn(n)==\"Reduce\"]\n",
    "    \n",
    "    \n",
    "    graph[\"nodes\"] = nodes\n",
    "    graph[\"channels\"] = channels\n",
    "    return graph\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recursive_explode = exploder([recursive_map, recursive_reduce])\n",
    "parallel_explode = exploder([lambda a, b: parallel_reduce(a, b, 1), \n",
    "                                         recursive_map, recursive_reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put together\n",
    "def fullfettle(filename, method, sizevar_file = None):\n",
    "    p = parse_file(filename, sizevar_file)\n",
    "    g = p['graph']\n",
    "    method(g)\n",
    "    add_dt_to_channels(p)\n",
    "    compose_maps(g)\n",
    "    add_dt_to_channels(p)\n",
    "    check_correct(g)\n",
    "    return p\n",
    "\n",
    "# Put together\n",
    "def get_rec_csdf(filename, sizevar_file = None):\n",
    "    return fullfettle(filename, recursive_explode, sizevar_file)\n",
    "def get_parallel_csdf(filename, sizevar_file = None):\n",
    "    return fullfettle(filename, parallel_explode, sizevar_file)\n",
    "\n",
    "def write_csdf_from_hl(from_file, to_file):\n",
    "    write_csdf(get_rec_csdf(from_file)[\"graph\"], to_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test():\n",
    "#     write_csdf_from_hl(add_cwd(\"highLevel/mmNN\"), add_cwd(\"csdf_xmls/mmNN\"))\n",
    "    \n",
    "def testr():\n",
    "    return get_rec_csdf(add_cwd(\"highLevel/mmNN\"))\n",
    "def testp():\n",
    "    return get_parallel_csdf(add_cwd(\"highLevel/mmNN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array (30, Float)\n"
     ]
    }
   ],
   "source": [
    "pp=testp()\n",
    "p=testr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array (30, Float)\n"
     ]
    }
   ],
   "source": [
    "gp = testp()['graph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = testr()['graph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n1\n",
      "[1, 1]\n",
      "n2\n",
      "[1, 1]\n",
      "n3\n",
      "[1, 1]\n",
      "n16\n",
      "[1, 1]\n",
      "n4\n",
      "[1, 1]\n",
      "n14\n",
      "[1, 1, 1]\n",
      "n8XRA\n",
      "[1, 1]\n",
      "n8XInitVal\n",
      "[1]\n",
      "n8XSpl\n",
      "[31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "n5_x0\n",
      "[1, 1, 1]\n",
      "n6_x0\n",
      "[1, 1]\n",
      "n7_x0\n",
      "[1, 1]\n",
      "n5_x1\n",
      "[1, 1, 1]\n",
      "n6_x1\n",
      "[1, 1]\n",
      "n7_x1\n",
      "[1, 1]\n",
      "n5_x2\n",
      "[1, 1, 1]\n",
      "n6_x2\n",
      "[1, 1]\n",
      "n7_x2\n",
      "[1, 1]\n",
      "n5_x3\n",
      "[1, 1, 1]\n",
      "n6_x3\n",
      "[1, 1]\n",
      "n7_x3\n",
      "[1, 1]\n",
      "n5_x4\n",
      "[1, 1, 1]\n",
      "n6_x4\n",
      "[1, 1]\n",
      "n7_x4\n",
      "[1, 1]\n",
      "n5_x5\n",
      "[1, 1, 1]\n",
      "n6_x5\n",
      "[1, 1]\n",
      "n7_x5\n",
      "[1, 1]\n",
      "n5_x6\n",
      "[1, 1, 1]\n",
      "n6_x6\n",
      "[1, 1]\n",
      "n7_x6\n",
      "[1, 1]\n",
      "n5_x7\n",
      "[1, 1, 1]\n",
      "n6_x7\n",
      "[1, 1]\n",
      "n7_x7\n",
      "[1, 1]\n",
      "n5_x8\n",
      "[1, 1, 1]\n",
      "n6_x8\n",
      "[1, 1]\n",
      "n7_x8\n",
      "[1, 1]\n",
      "n5_x9\n",
      "[1, 1, 1]\n",
      "n6_x9\n",
      "[1, 1]\n",
      "n7_x9\n",
      "[1, 1]\n",
      "n5_x10\n",
      "[1, 1, 1]\n",
      "n6_x10\n",
      "[1, 1]\n",
      "n7_x10\n",
      "[1, 1]\n",
      "n5_x11\n",
      "[1, 1, 1]\n",
      "n6_x11\n",
      "[1, 1]\n",
      "n7_x11\n",
      "[1, 1]\n",
      "n5_x12\n",
      "[1, 1, 1]\n",
      "n6_x12\n",
      "[1, 1]\n",
      "n7_x12\n",
      "[1, 1]\n",
      "n5_x13\n",
      "[1, 1, 1]\n",
      "n6_x13\n",
      "[1, 1]\n",
      "n7_x13\n",
      "[1, 1]\n",
      "n5_x14\n",
      "[1, 1, 1]\n",
      "n6_x14\n",
      "[1, 1]\n",
      "n7_x14\n",
      "[1, 1]\n",
      "n5_x15\n",
      "[1, 1, 1]\n",
      "n6_x15\n",
      "[1, 1]\n",
      "n7_x15\n",
      "[1, 1]\n",
      "n5_x16\n",
      "[1, 1, 1]\n",
      "n6_x16\n",
      "[1, 1]\n",
      "n7_x16\n",
      "[1, 1]\n",
      "n5_x17\n",
      "[1, 1, 1]\n",
      "n6_x17\n",
      "[1, 1]\n",
      "n7_x17\n",
      "[1, 1]\n",
      "n5_x18\n",
      "[1, 1, 1]\n",
      "n6_x18\n",
      "[1, 1]\n",
      "n7_x18\n",
      "[1, 1]\n",
      "n5_x19\n",
      "[1, 1, 1]\n",
      "n6_x19\n",
      "[1, 1]\n",
      "n7_x19\n",
      "[1, 1]\n",
      "n5_x20\n",
      "[1, 1, 1]\n",
      "n6_x20\n",
      "[1, 1]\n",
      "n7_x20\n",
      "[1, 1]\n",
      "n5_x21\n",
      "[1, 1, 1]\n",
      "n6_x21\n",
      "[1, 1]\n",
      "n7_x21\n",
      "[1, 1]\n",
      "n5_x22\n",
      "[1, 1, 1]\n",
      "n6_x22\n",
      "[1, 1]\n",
      "n7_x22\n",
      "[1, 1]\n",
      "n5_x23\n",
      "[1, 1, 1]\n",
      "n6_x23\n",
      "[1, 1]\n",
      "n7_x23\n",
      "[1, 1]\n",
      "n5_x24\n",
      "[1, 1, 1]\n",
      "n6_x24\n",
      "[1, 1]\n",
      "n7_x24\n",
      "[1, 1]\n",
      "n5_x25\n",
      "[1, 1, 1]\n",
      "n6_x25\n",
      "[1, 1]\n",
      "n7_x25\n",
      "[1, 1]\n",
      "n5_x26\n",
      "[1, 1, 1]\n",
      "n6_x26\n",
      "[1, 1]\n",
      "n7_x26\n",
      "[1, 1]\n",
      "n5_x27\n",
      "[1, 1, 1]\n",
      "n6_x27\n",
      "[1, 1]\n",
      "n7_x27\n",
      "[1, 1]\n",
      "n5_x28\n",
      "[1, 1, 1]\n",
      "n6_x28\n",
      "[1, 1]\n",
      "n7_x28\n",
      "[1, 1]\n",
      "n5_x29\n",
      "[1, 1, 1]\n",
      "n6_x29\n",
      "[1, 1]\n",
      "n7_x29\n",
      "[1, 1]\n",
      "n9\n",
      "[1, 1, 1]\n",
      "n11\n",
      "[1, 1]\n",
      "n12\n",
      "[1, 1]\n",
      "n10\n",
      "[1, 1, 1]\n",
      "n13XDA\n",
      "[1, 1]\n",
      "n15XDA\n",
      "[1, 1]\n",
      "n15XRA\n",
      "[1, 1]\n",
      "n17XDA\n",
      "[1, 1]\n",
      "n17XRA\n",
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "write_csdf(gp, 'csdf_xmls/mmnn_parallel_30.xml')\n",
    "# write_csdf(gr, 'csdf_xmls/mmnn_rec_30.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
